
> library("betareg")

> knitr::opts_chunk$set(engine = "R", collapse = TRUE, 
+     comment = "##", message = FALSE, warning = FALSE, echo = TRUE)

> options(width = 70, prompt = "R> ", continue = "+  ", 
+     useFancyQuotes = FALSE, digits = 5)

> combine <- function(x, sep, width) {
+     cs <- cumsum(nchar(x))
+     remaining <- if (any(cs[-1] > width)) 
+         combine(x[c(FALSE, cs[-1] > .... [TRUNCATED] 

> prettyPrint <- function(x, sep = " ", linebreak = "\n\t", 
+     width = getOption("width")) {
+     x <- strsplit(x, sep)[[1]]
+     paste(combine( .... [TRUNCATED] 

> cache <- FALSE

> enumerate <- function(x) paste(paste(x[-length(x)], 
+     collapse = ", "), x[length(x)], sep = " and ")

> betamix_methods <- enumerate(paste("`", gsub("\\.betamix", 
+     "", as.character(methods(class = "betamix"))), "`", sep = ""))

> data("ReadingSkills", package = "betareg")

> mean_accuracy <- format(round(with(ReadingSkills, 
+     tapply(accuracy, dyslexia, mean)), digits = 3), nsmall = 3)

> mean_iq <- format(round(with(ReadingSkills, tapply(iq, 
+     dyslexia, mean)), digits = 3), nsmall = 3)

> data("ReadingSkills", package = "betareg")

> rs_ols <- lm(qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)

> rs_beta <- betareg(accuracy ~ dyslexia * iq | dyslexia + 
+     iq, data = ReadingSkills, hessian = TRUE)

> cl1 <- hcl(c(260, 0), 90, 40)

> cl2 <- hcl(c(260, 0), 10, 95)

> plot(accuracy ~ iq, data = ReadingSkills, col = cl2[as.numeric(dyslexia)], 
+     main = "Reading skills data", xlab = "IQ score", ylab = "Reading a ..." ... [TRUNCATED] 

> points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, 
+     pch = (1:2)[as.numeric(dyslexia)], col = cl1[as.numeric(dyslexia)])

> nd <- data.frame(dyslexia = "no", iq = -30:30/10)

> lines(nd$iq, predict(rs_beta, nd), col = cl1[1], lwd = 2)

> lines(nd$iq, plogis(predict(rs_ols, nd)), col = cl1[1], 
+     lty = 2, lwd = 2)

> nd <- data.frame(dyslexia = "yes", iq = -30:30/10)

> lines(nd$iq, predict(rs_beta, nd), col = cl1[2], lwd = 2)

> lines(nd$iq, plogis(predict(rs_ols, nd)), col = cl1[2], 
+     lty = 2, lwd = 2)

> legend("topleft", c("control", "dyslexic", "betareg", 
+     "lm"), lty = c(NA, NA, 1:2), pch = c(19, 17, NA, NA), lwd = 2, 
+     col = c(cl2, 1, 1 .... [TRUNCATED] 

> legend("topleft", c("control", "dyslexic", "betareg", 
+     "lm"), lty = c(NA, NA, 1:2), pch = c(1, 2, NA, NA), col = c(cl1, 
+     NA, NA), bty =  .... [TRUNCATED] 

> data("ReadingSkills", package = "betareg")

> rs_f <- accuracy ~ dyslexia * iq | dyslexia * iq

> rs_ml <- betareg(rs_f, data = ReadingSkills, type = "ML")

> rs_bc <- betareg(rs_f, data = ReadingSkills, type = "BC")

> rs_br <- betareg(rs_f, data = ReadingSkills, type = "BR")

> rs_list <- list(rs_ml, rs_bc, rs_br)

> cf <- paste("$", sapply(round(sapply(rs_list, coef), 
+     digits = 3), format, nsmall = 3), "$", sep = "")

> se <- paste("$", format(round(sapply(rs_list, function(x) sqrt(diag(vcov(x)))), 
+     digits = 3), nsmall = 3), "$", sep = "")

> ll <- paste("$", format(round(sapply(rs_list, logLik), 
+     digits = 3), nsmall = 3), "$", sep = "")

> cfse <- matrix(as.vector(rbind(cf, se)), ncol = 3)

> cfse <- cbind(c("Mean", rep("", 7), "Precision", rep("", 
+     7)), rep(as.vector(rbind(c("(Intercept)", "`dyslexia`", "`iq`", 
+     "`dyslexia:iq ..." ... [TRUNCATED] 

> cfse <- rbind(cfse, c("Log-likelihood", "", ll))

> knitr::kable(cfse, align = c("l", "l", "r", "r", "r"), 
+     col.names = c("", "", "Maximum likelihood", "Bias correction", 
+         "Bias reduct ..." ... [TRUNCATED] 


|               |              | Maximum likelihood| Bias correction| Bias reduction|
|:--------------|:-------------|------------------:|---------------:|--------------:|
|Mean           |(Intercept)   |            $1.019$|         $0.990$|        $0.985$|
|               |              |            $0.145$|         $0.150$|        $0.150$|
|               |`dyslexia`    |           $-0.638$|        $-0.610$|       $-0.603$|
|               |              |            $0.145$|         $0.150$|        $0.150$|
|               |`iq`          |            $0.690$|         $0.700$|        $0.707$|
|               |              |            $0.127$|         $0.133$|        $0.133$|
|               |`dyslexia:iq` |           $-0.776$|        $-0.786$|       $-0.784$|
|               |              |            $0.127$|         $0.133$|        $0.133$|
|Precision      |(Intercept)   |            $3.040$|         $2.811$|        $2.721$|
|               |              |            $0.258$|         $0.257$|        $0.256$|
|               |`dyslexia`    |            $1.768$|         $1.705$|        $1.634$|
|               |              |            $0.258$|         $0.257$|        $0.256$|
|               |`iq`          |            $1.437$|         $1.370$|        $1.281$|
|               |              |            $0.257$|         $0.257$|        $0.257$|
|               |`dyslexia:iq` |           $-0.611$|        $-0.668$|       $-0.759$|
|               |              |            $0.257$|         $0.257$|        $0.257$|
|Log-likelihood |              |           $66.734$|        $66.334$|       $66.134$|

> pr_phi <- sapply(list(`Maximum likelihood` = rs_ml, 
+     `Bias correction` = rs_bc, `Bias reduction` = rs_br), predict, 
+     type = "precision")

> pairs(log(pr_phi), panel = function(x, y, ...) {
+     panel.smooth(x, y, ...)
+     abline(0, 1, lty = 2)
+ })

> suppressWarnings(RNGversion("3.5.0"))

> set.seed(1071)

> n <- nrow(ReadingSkills)

> ReadingSkills$x1 <- rnorm(n)

> ReadingSkills$x2 <- runif(n)

> ReadingSkills$x3 <- factor(sample(0:1, n, replace = TRUE))

> rs_tree <- betatree(accuracy ~ iq | iq, ~dyslexia + 
+     x1 + x2 + x3, data = ReadingSkills, minsize = 10)

> plot(rs_tree)

> coef(rs_tree)
  (Intercept)        iq (phi)_(Intercept) (phi)_iq
2     1.65653  1.465708            1.2726  2.04786
3     0.38093 -0.086228            4.8077  0.82603

> rs_tree
Beta regression tree

Model formula:
accuracy ~ iq + iq | dyslexia + x1 + x2 + x3

Fitted party:
[1] root
|   [2] dyslexia in no: n = 25
|             (Intercept)                iq (phi)_(Intercept) 
|                  1.6565            1.4657            1.2726 
|                (phi)_iq 
|                  2.0479 
|   [3] dyslexia in yes: n = 19
|             (Intercept)                iq (phi)_(Intercept) 
|                0.380932         -0.086228          4.807662 
|                (phi)_iq 
|                0.826033 

Number of inner nodes:    1
Number of terminal nodes: 2
Number of parameters per node: 4
Objective function (negative log-likelihood): -66.734

> library("strucchange")
Loading required package: zoo

Attaching package: 'zoo'

The following objects are masked from 'package:base':

    as.Date, as.Date.numeric

Loading required package: sandwich

> sctest(rs_tree)
$`1`
            dyslexia      x1      x2     x3
statistic 2.2687e+01 8.52510 5.56986 3.6273
p.value   5.8479e-04 0.90946 0.99871 0.9142

$`2`
          dyslexia      x1      x2      x3
statistic        0 6.41163 4.51702 8.20191
p.value         NA 0.84121 0.97516 0.23257

$`3`
NULL


> rs_mix <- betamix(accuracy ~ iq, data = ReadingSkills, 
+     k = 3, extra_components = extraComponent(type = "uniform", 
+         coef = 0.99, del .... [TRUNCATED] 

> rs_mix

Call:
betamix(formula = accuracy ~ iq, data = ReadingSkills, 
    k = 3, nstart = 10, extra_components = extraComponent(type = "uniform", 
        coef = 0.99, delta = 0.01))

Cluster sizes:
 1  2  3 
20 10 14 

convergence after 20 iterations

> summary(rs_mix)
$Comp.1
$Comp.1$mean
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)   0.5025     0.0825    6.09  1.1e-09 ***
iq           -0.0484     0.1130   -0.43     0.67    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

$Comp.1$precision
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)    4.251      0.748    5.69  1.3e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


$Comp.2
$Comp.2$mean
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)    1.403      0.263    5.33  9.9e-08 ***
iq             0.825      0.216    3.81  0.00014 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

$Comp.2$precision
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)    2.685      0.454    5.91  3.4e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



> par(mfrow = c(1, 2))

> ix <- as.numeric(ReadingSkills$dyslexia)

> prob <- 2 * (posterior(rs_mix)[cbind(seq_along(ix), 
+     clusters(rs_mix))] - 0.5)

> col3 <- hcl(c(0, 260, 130), 65, 45, fixup = FALSE)

> col1 <- col3[clusters(rs_mix)]

> col2 <- hcl(c(0, 260, 130)[clusters(rs_mix)], 65 * 
+     abs(prob)^1.5, 95 - 50 * abs(prob)^1.5, fixup = FALSE)

> plot(accuracy ~ iq, data = ReadingSkills, col = col2, 
+     pch = 19, cex = 1.5, xlim = c(-2, 2), main = "Mixture model (dyslexia unobserved)")

> points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, 
+     pch = 1, col = col1)

> iq <- -30:30/10

> cf <- rbind(coef(rs_mix, model = "mean", component = 1:2), 
+     c(qlogis(0.99), 0))

> for (i in 1:3) lines(iq, plogis(cf[i, 1] + cf[i, 2] * 
+     iq), lwd = 2, col = col3[i])

> ix <- as.numeric(ReadingSkills$dyslexia)

> col1 <- hcl(c(260, 0), 90, 40)[ix]

> col2 <- hcl(c(260, 0), 10, 95)[ix]

> plot(accuracy ~ iq, data = ReadingSkills, col = col2, 
+     pch = 19, cex = 1.5, xlim = c(-2, 2), main = "Partitioned model (dyslexia observed)")

> points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, 
+     pch = 1, col = col1)

> cf <- coef(rs_tree, model = "mean")

> col3 <- hcl(c(260, 0), 90, 40)

> for (i in 1:2) lines(iq, plogis(cf[i, 1] + cf[i, 2] * 
+     iq), lwd = 2, col = col3[i])

> table(clusters(rs_mix), ReadingSkills$dyslexia)
   
    no yes
  1  4  16
  2  7   3
  3 14   0

> data("GasolineYield", package = "betareg")

> gy <- lapply(c("ML", "BC", "BR"), function(x) betareg(yield ~ 
+     batch + temp, data = GasolineYield, type = x))

> cf <- matrix(paste("$", sapply(round(sapply(gy, coef), 
+     digits = 5), format, nsmall = 5), "$", sep = ""), ncol = 3)

> se <- matrix(gsub(" ", "", paste("$", format(round(sapply(gy, 
+     function(x) sqrt(diag(vcov(x)))), digits = 5), nsmall = 5), 
+     "$", sep = " ..." ... [TRUNCATED] 

> cfse <- cbind(c(paste("$\\beta_{", 1:11, "}$", sep = ""), 
+     "$\\phi$"), cf[, 1], se[, 1], cf[, 2], se[, 2], cf[, 3], 
+     se[, 3])

> knitr::kable(cfse, align = c("l", rep("r", 6)), col.names = c("", 
+     "Maximum likelihood", "", "Bias correction", "", "Bias reduction", 
+     " ..." ... [TRUNCATED] 


|             | Maximum likelihood|            | Bias correction|           | Bias reduction|           |
|:------------|------------------:|-----------:|---------------:|----------:|--------------:|----------:|
|$\beta_{1}$  |         $-6.15957$|   $0.18232$|      $-6.14837$|  $0.23595$|     $-6.14171$|  $0.23588$|
|$\beta_{2}$  |          $1.72773$|   $0.10123$|       $1.72484$|  $0.13107$|      $1.72325$|  $0.13106$|
|$\beta_{3}$  |          $1.32260$|   $0.11790$|       $1.32009$|  $0.15260$|      $1.31860$|  $0.15257$|
|$\beta_{4}$  |          $1.57231$|   $0.11610$|       $1.56928$|  $0.15030$|      $1.56734$|  $0.15028$|
|$\beta_{5}$  |          $1.05971$|   $0.10236$|       $1.05788$|  $0.13251$|      $1.05677$|  $0.13249$|
|$\beta_{6}$  |          $1.13375$|   $0.10352$|       $1.13165$|  $0.13404$|      $1.13024$|  $0.13403$|
|$\beta_{7}$  |          $1.04016$|   $0.10604$|       $1.03829$|  $0.13729$|      $1.03714$|  $0.13727$|
|$\beta_{8}$  |          $0.54369$|   $0.10913$|       $0.54309$|  $0.14119$|      $0.54242$|  $0.14116$|
|$\beta_{9}$  |          $0.49590$|   $0.10893$|       $0.49518$|  $0.14099$|      $0.49446$|  $0.14096$|
|$\beta_{10}$ |          $0.38579$|   $0.11859$|       $0.38502$|  $0.15353$|      $0.38459$|  $0.15351$|
|$\beta_{11}$ |          $0.01097$|   $0.00041$|       $0.01094$|  $0.00053$|      $0.01093$|  $0.00053$|
|$\phi$       |        $440.27839$| $110.02562$|     $261.20610$| $65.25866$|    $261.03777$| $65.21640$|

> sapply(gy, coef, model = "precision")
 (phi)  (phi)  (phi) 
440.28 261.21 261.04 

> sapply(gy, logLik)
[1] 84.798 82.947 82.945

> data("GasolineYield", package = "betareg")

> gy2 <- lapply(c("ML", "BC", "BR"), function(x) betareg(yield ~ 
+     batch + temp | 1, data = GasolineYield, type = x))

> sapply(gy2, logLik)
[1] 84.798 83.797 83.268

> cf <- matrix(paste("$", sapply(round(sapply(gy2, coef), 
+     digits = 5), format, nsmall = 5), "$", sep = ""), ncol = 3)

> se <- matrix(gsub(" ", "", paste("$", format(round(sapply(gy2, 
+     function(x) sqrt(diag(vcov(x)))), digits = 5), nsmall = 5), 
+     "$", sep =  .... [TRUNCATED] 

> cfse <- cbind(c(paste("$\\beta_{", 1:11, "}$", sep = ""), 
+     "$\\log\\phi$"), cf[, 1], se[, 1], cf[, 2], se[, 2], cf[, 
+     3], se[, 3])

> knitr::kable(cfse, align = c("l", rep("r", 6)), col.names = c("", 
+     "Maximum likelihood", "", "Bias correction", "", "Bias reduction", 
+     " ..." ... [TRUNCATED] 


|             | Maximum likelihood|          | Bias correction|          | Bias reduction|          |
|:------------|------------------:|---------:|---------------:|---------:|--------------:|---------:|
|$\beta_{1}$  |         $-6.15957$| $0.18232$|      $-6.14837$| $0.21944$|     $-6.14259$| $0.22998$|
|$\beta_{2}$  |          $1.72773$| $0.10123$|       $1.72484$| $0.12189$|      $1.72347$| $0.12777$|
|$\beta_{3}$  |          $1.32260$| $0.11790$|       $1.32009$| $0.14193$|      $1.31880$| $0.14875$|
|$\beta_{4}$  |          $1.57231$| $0.11610$|       $1.56928$| $0.13978$|      $1.56758$| $0.14651$|
|$\beta_{5}$  |          $1.05971$| $0.10236$|       $1.05788$| $0.12323$|      $1.05691$| $0.12917$|
|$\beta_{6}$  |          $1.13375$| $0.10352$|       $1.13165$| $0.12465$|      $1.13041$| $0.13067$|
|$\beta_{7}$  |          $1.04016$| $0.10604$|       $1.03829$| $0.12767$|      $1.03729$| $0.13383$|
|$\beta_{8}$  |          $0.54369$| $0.10913$|       $0.54309$| $0.13133$|      $0.54248$| $0.13763$|
|$\beta_{9}$  |          $0.49590$| $0.10893$|       $0.49518$| $0.13112$|      $0.49453$| $0.13743$|
|$\beta_{10}$ |          $0.38579$| $0.11859$|       $0.38502$| $0.14278$|      $0.38465$| $0.14966$|
|$\beta_{11}$ |          $0.01097$| $0.00041$|       $0.01094$| $0.00050$|      $0.01093$| $0.00052$|
|$\log\phi$   |          $6.08741$| $0.24990$|       $5.71191$| $0.24986$|      $5.61608$| $0.24984$|

 *** Run successfully completed ***
> proc.time()
   user  system elapsed 
  6.240   0.115   6.352 
